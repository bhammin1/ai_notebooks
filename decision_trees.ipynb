{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "1. Change the name of this file to be your JHED id as in `jsmith299.ipynb`. Because sure you use your JHED ID (it's made out of your name and not your student id which is just letters and numbers).\n",
    "2. Make sure the notebook you submit is cleanly and fully executed. I do not grade unexecuted notebooks.\n",
    "3. Submit your notebook back in Blackboard where you downloaded this file.\n",
    "\n",
    "*Provide the output **exactly** as requested*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import List, Dict, Tuple, Callable\n",
    "import random\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "For this assignment you will be implementing and evaluating a Decision Tree using the ID3 Algorithm (**no** pruning or normalized information gain). Use the provided pseudocode. The data is located at (copy link):\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "**Just in case** the UCI repository is down, which happens from time to time, I have included the data and name files on Blackboard.\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        No Pandas. The only acceptable libraries in this class are those contained in the `environment.yml`. No OOP, either. You can used Dicts, NamedTuples, etc. as your abstract data type (ADT) for the the tree and nodes.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "One of the things we did not talk about in the lectures was how to deal with missing values. There are two aspects of the problem here. What do we do with missing values in the training data? What do we do with missing values when doing classifcation?\n",
    "\n",
    "For the first problem, C4.5 handled missing values in an interesting way. Suppose we have identifed some attribute *B* with values {b1, b2, b3} as the best current attribute. Furthermore, assume there are 5 observations with B=?, that is, we don't know the attribute value. In C4.5, those 5 observations would be added to *all* of the subsets created by B=b1, B=b2, B=b3 with decreased weights. Note that the observations with missing values are not part of the information gain calculation.\n",
    "\n",
    "This doesn't quite help us if we have missing values when we use the model. What happens if we have missing values during classification? One approach is to prepare for this advance. When you train the tree, you need to add an implicit attribute value \"?\" at every split. For example, if the attribute was \"size\" then the domain would be [\"small\", \"medium\", \"large\", \"?\"]. The \"?\" value gets all the data (because ? is now a wildcard). However, there is an issue with this approach. \"?\" becomes the worst possible attribut value because it has no classification value. What to do? There are several options:\n",
    "\n",
    "1. Never recurse on \"?\" if you do not also recurse on at least one *real* attribute value.\n",
    "2. Limit the depth of the tree.\n",
    "\n",
    "There are good reasons, in general, to limit the depth of a decision tree because they tend to overfit.\n",
    "Otherwise, the algorithm *will* exhaust all the attributes trying to fulfill one of the base cases.\n",
    "\n",
    "You must implement the following functions:\n",
    "\n",
    "`train` takes training_data and returns the Decision Tree as a data structure. There are many options including namedtuples and just plain old nested dictionaries. **No OOP**.\n",
    "\n",
    "```\n",
    "def train(training_data, depth_limit=None):\n",
    "   # returns the Decision Tree.\n",
    "```\n",
    "\n",
    "The `depth_limit` value defaults to None. (What technique would we use to determine the best parameter value for `depth_limit` hint: Module 3!)\n",
    "\n",
    "`classify` takes a tree produced from the function above and applies it to labeled data (like the test set) or unlabeled data (like some new data).\n",
    "\n",
    "```\n",
    "def classify(tree, observations, labeled=True):\n",
    "    # returns a list of classifications\n",
    "```\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the classification result and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "Do not use anything else as evaluation metric or the submission will be deemed incomplete, ie, an \"F\". (Hint: accuracy rate is not the error rate!).\n",
    "\n",
    "`cross_validate` takes the data and uses 10 fold cross validation (from Module 3!) to `train`, `classify`, and `evaluate`. **Remember to shuffle your data before you create your folds**. I leave the exact signature of `cross_validate` to you but you should write it so that you can use it with *any* `classify` function of the same form (using higher order functions and partial application).\n",
    "\n",
    "Following Module 3's discussion, `cross_validate` should print out the fold number and the evaluation metric (error rate) for each fold and then the average value (and the variance). What you are looking for here is a consistent evaluation metric cross the folds. You should print the error rates in terms of percents (ie, multiply the error rate by 100 and add \"%\" to the end).\n",
    "\n",
    "```\n",
    "def pretty_print_tree(tree):\n",
    "    # pretty prints the tree\n",
    "```\n",
    "\n",
    "This should be a text representation of a decision tree trained on the entire data set (no train/test).\n",
    "\n",
    "To summarize...\n",
    "\n",
    "Apply the Decision Tree algorithm to the Mushroom data set using 10 fold cross validation and the error rate as the evaluation metric. When you are done, apply the Decision Tree algorithm to the entire data set and print out the resulting tree.\n",
    "\n",
    "**Note** Because this assignment has a natural recursive implementation, you should consider using `deepcopy` at the appropriate places.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data ready\n",
    "mushroom_cols = [\n",
    "    \"cap-shape\"\n",
    "    ,\"cap-surface\"\n",
    "    ,\"cap-color\"\n",
    "    ,\"bruises\"\n",
    "    ,\"odor\"\n",
    "    ,\"gill-attachment\"\n",
    "    ,\"gill-spacing\"\n",
    "    ,\"gill-size\"\n",
    "    ,\"gill-color\"\n",
    "    ,\"stalk-shape\"\n",
    "    ,\"stalk-root\"\n",
    "    ,\"stalk-surface-above-ring\"\n",
    "    ,\"stalk-surface-below-ring\"\n",
    "    ,\"stalk-color-above-ring\"\n",
    "    ,\"stalk-color-below-ring\"\n",
    "    ,\"veil-type\"\n",
    "    ,\"veil-color\"\n",
    "    ,\"ring-number\"\n",
    "    ,\"ring-type\"\n",
    "    ,\"spore-print-color\"\n",
    "    ,\"population\"\n",
    "    ,\"habitat\"\n",
    "    ,\"edibility\"\n",
    "]\n",
    "\n",
    "self_check =[[ 'Shape', 'Size', 'Color', 'Safe?'],\n",
    " ['round', 'large', 'blue', 'no'],\n",
    " [ 'square', 'large', 'red', 'no'],\n",
    " ['round', 'large', 'green', 'yes'],\n",
    " ['square', 'large', 'green', 'yes'],\n",
    " [ 'square', 'large', 'green', 'yes'],\n",
    " [ 'square', 'large', 'green', 'yes'],\n",
    " ['round', 'large', 'red', 'yes'],\n",
    " [ 'round', 'large', 'red', 'yes'],\n",
    " [ 'round', 'small', 'blue', 'no'],\n",
    " ['square', 'small', 'blue', 'no'],\n",
    " ['round', 'small', 'green', 'no'],\n",
    " [ 'square', 'small', 'green', 'no'],\n",
    " ['square', 'small', 'red', 'no'],\n",
    " [ 'square', 'small', 'red', 'no'],\n",
    " ['round', 'small', 'red', 'yes']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"parse_data\"></a>\n",
    "## parse_data\n",
    "\n",
    "- Reads in a comma separated file into a nested list\n",
    "- Stores the label column as the very last column\n",
    "- Function mostly resued from mod 3\n",
    "\n",
    "* **file_name** str: path to where file is located\n",
    "* **class_index** int: index of where label field is in the file\n",
    "\n",
    "**returns** List[List[]]: data stored in a nest list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file_name: str, class_index:int) -> List[List]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = [value for value in line.rstrip().split(\",\")]\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    for row in data:\n",
    "        #swap\n",
    "        label = row[class_index]\n",
    "        row.pop(class_index)\n",
    "        row.append(label)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit tests\n",
    "data = parse_data(\"agaricus-lepiota.data\",0)\n",
    "\n",
    "# verify all observations are present\n",
    "assert len(data) ==8124 \n",
    "\n",
    "#verify all attributes and class cols are present\n",
    "assert len(data[0]) == 23\n",
    "\n",
    "# verify moved class/label col is last column\n",
    "for row in data[1:]:\n",
    "    assert row[0] not in ['p','e'] # first col is cap-shape, doesnt have values e or p\n",
    "    assert row[-1] in ['p','e'] # label/class only takes e or p value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create_folds\"></a>\n",
    "## create_folds\n",
    "\n",
    "- Resued from mod 3\n",
    "- Creates folds from the data. Fold number based on parameter\n",
    "\n",
    "* **xs** List[List[]]: list of to perform cross validation on\n",
    "* **n** int: number of folds\n",
    "\n",
    "**returns** List[List[float]]: normalized data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(xs: List[List], n: int) -> List[List[List]]:\n",
    "    k, m = divmod(len(xs), n)\n",
    "    # be careful of generators...\n",
    "    return list(xs[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make unit tests\n",
    "folds = create_folds(data, 10)\n",
    "\n",
    "#verify a list is returned\n",
    "assert type(folds) == list\n",
    "\n",
    "# no other unit tests since this was used in mod 3. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create_train_test\"></a>\n",
    "## create_train_test\n",
    "\n",
    "- Mostly resused function from Mod 3\n",
    "- Creates training and test data based on folds\n",
    "- also for both training and test set, the column names are added to the first(index 0) row of the list\n",
    "\n",
    "* **folds**: List[List[List]]: data to split\n",
    "* **index** : index of fold for splitting\n",
    "* **cols_names**: list of column names\n",
    "\n",
    "**returns** Tuple[List[List], List[List]]: returns training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(folds: List[List[List]], index: int, cols_names:List[str]) -> Tuple[List[List], List[List]]:\n",
    "    training = []\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = training + fold\n",
    "    # add column names\n",
    "    training.insert(0,cols_names)\n",
    "    test_copy = deepcopy(test)\n",
    "    test_copy.insert(0,cols_names)\n",
    "    return training, test_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = create_train_test(folds, 0,mushroom_cols)\n",
    "\n",
    "# verify first row is the column names\n",
    "assert train[0] == test[0] == mushroom_cols\n",
    "\n",
    "# verify train is 9/10 of data\n",
    "assert len(train) == math.floor(len(data) * 9/10) +1 # round for the col name\n",
    "\n",
    "\n",
    "# train and test should be the same size as data\n",
    "assert len(test) + len(train) == len(data) +2 # 2 col rows\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"entropy\"></a>\n",
    "## entropy\n",
    "\n",
    "- Calculates the entropy of a given data set\n",
    "- assumes that the first row in the data set only contains col names. Not data values\n",
    "- And assumed that the label field is the in the last column of the data\n",
    "\n",
    "* **data** List[List[]]: data used for classifying \n",
    "\n",
    "**returns** float: returns the entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(data:List[List])->float:\n",
    "    label_data = [ row[-1] for row in data[1:]]\n",
    "\n",
    "    n = len(label_data)\n",
    "    p_vals = set(label_data) # can work for more than just binary data\n",
    "    e = 0\n",
    "\n",
    "    for p in p_vals:\n",
    "        p_cnt = len([ v for v in label_data if v ==p])\n",
    "        p_i = p_cnt/n\n",
    "\n",
    "        e = e - p_i * math.log(p_i,2)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_e = entropy(self_check)\n",
    "test_e\n",
    "\n",
    "# verify the entropy is correct \n",
    "assert round(test_e) == 1\n",
    "\n",
    "# verify entropy can handle none binary data\n",
    "self_check_mod = deepcopy(self_check)\n",
    "self_check_mod.append(['7', 'round', 'small', 'red', 'unknown'])\n",
    "test_e2 = entropy(self_check_mod)\n",
    "assert test_e2 > 0\n",
    "assert test_e2 != test_e\n",
    "\n",
    "# if empty dataset, 0 is returned\n",
    "test_e3 = entropy([])\n",
    "assert test_e3 == 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"info_gain\"></a>\n",
    "## info_gain\n",
    "\n",
    "- Calculates the information gain for a given attribute\n",
    "\n",
    "* **data** List[List[]]: data used for classifying \n",
    "* **col_index** int: index for the attribute to calculate info gain on\n",
    "\n",
    "**returns** float: returns the entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(data:List[List],col_index:int)->float:\n",
    "    e = entropy(data)\n",
    "    \n",
    "    col_vals = [ row[col_index] for row in data]\n",
    "    col_unique = set(col_vals[1:])\n",
    "    col_name = data[0][col_index]\n",
    "    n = len(data[1:])\n",
    "    gain_info = e\n",
    "\n",
    "    for c in col_unique:\n",
    "        data_col = [row for row in data if row[col_index]== c or row[col_index]==col_name]\n",
    "        col_e = entropy(data_col)\n",
    "        rel_freq =(len(data_col) -1) / n # subtract col name row\n",
    "        gain_info += - rel_freq * col_e\n",
    "    \n",
    "    return gain_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_gain = info_gain(self_check,1)\n",
    "size_gain =  info_gain(self_check,2)\n",
    "size_color = info_gain(self_check,3)\n",
    "e = entropy(self_check)\n",
    "\n",
    "#verify info gain is different than entropy\n",
    "assert shape_gain != e\n",
    "\n",
    "# verify gain is different between the two values\n",
    "assert shape_gain != size_gain\n",
    "\n",
    "# verify no error is thrown when a homogeneous entropy is present\n",
    "# when a dominate value, log of 0 would throw an error\n",
    "assert size_color >0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"best_attribute\"></a>\n",
    "## best_attribute\n",
    "\n",
    "- Finds the attribute with the highest gain\n",
    "- This is done by finding the info gain for each remianing col\n",
    "- Returns the col name and col index with the highest gain\n",
    "\n",
    "* **data** List[List[]]: data used for classifying \n",
    "* **attributes** List[str]: list of column names available to select from\n",
    "\n",
    "**returns** Tuple[str, int]: column name and index with highest gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_attribute(data:List[List], attributes:List[str])->Tuple[str, int]:\n",
    "    gains= [] # gain value in order of attribute index value\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "    #list of indexes of attributes   \n",
    "    attr_index = [data[0].index(col) for col in data[0] if col in attributes]\n",
    "\n",
    "    for col_index in attr_index:\n",
    "        col_gain = info_gain(data,col_index)\n",
    "        gains.append(col_gain)\n",
    "    \n",
    "    # find max\n",
    "    best_gain_index = attr_index[gains.index(max(gains))]\n",
    "    return data[0][best_gain_index], best_gain_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_self_check, root_index = best_attribute(self_check, [\"Size\",\"Shape\",\"Color\"])\n",
    "\n",
    "#verify col name not index is returned\n",
    "assert isinstance(root_self_check, str)\n",
    "\n",
    "# verify index is returned\n",
    "assert root_index == 1\n",
    "\n",
    "# verify size is returned\n",
    "assert root_self_check ==\"Size\"\n",
    "\n",
    "# verify None is returned if data is empty, error checking\n",
    "empty_val = best_attribute([],[])\n",
    "assert empty_val == None\n",
    "\n",
    "#verify best attribute found if only have two cols\n",
    "root_self_check1, root_index1 = best_attribute(self_check, [\"Shape\",\"Color\"])\n",
    "assert root_self_check1 ==\"Color\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"homogen\"></a>\n",
    "## homogen\n",
    "\n",
    "- Indicates where a data set is homogeneuos or not\n",
    "- Returns the values the label takes if homogenous.\n",
    "- If data is not homogeneous, then None is returned\n",
    "\n",
    "* **data** List[List[]]: data used for classifying \n",
    "\n",
    "**returns** str: if homogeneous, then value of label else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homogen(data:List[List])->str:\n",
    "    if len(data) <=1 :\n",
    "        return None\n",
    "    label_data = [ row[-1] for row in data[1:]]\n",
    "    unique_labels = set(label_data)\n",
    "\n",
    "    if len(unique_labels) ==1:\n",
    "        return unique_labels.pop()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = homogen(self_check)\n",
    "\n",
    "# verify returns None when not homogeneous\n",
    "assert h1 == None\n",
    "\n",
    "#verify returns the value for homogenous set\n",
    "h2 = homogen(self_check[:2])\n",
    "assert h2 == 'no'\n",
    "\n",
    "# verify none returned if data set only contains columns\n",
    "h3 = homogen(self_check[:1])\n",
    "assert h3 == None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"majority_label\"></a>\n",
    "## majority_label\n",
    "\n",
    "- Finds the label with the highest frequncy in the data set\n",
    "\n",
    "* **data** List[List[]]: data used for classifying \n",
    "\n",
    "**returns** str: value of majority label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_label(data:List[List])->str:\n",
    "    if len(data) <=1:\n",
    "        return None\n",
    "    labels = [ row[-1] for row in data[1:]]\n",
    "    unique_labels = set(labels)\n",
    "    max_cnt =0\n",
    "    max_label = None\n",
    "\n",
    "    for l in unique_labels:\n",
    "        current_cnt = labels.count(l)\n",
    "        if current_cnt > max_cnt:\n",
    "            max_cnt = current_cnt\n",
    "            max_label = l\n",
    "    return max_label  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_check_major = majority_label(self_check)\n",
    "\n",
    "#verify value not index is returned\n",
    "assert isinstance(self_check_major,str)\n",
    "\n",
    "#verify the correct answer is returned\n",
    "assert self_check_major == 'no'\n",
    "\n",
    "#verify none is returned if no real data\n",
    "self_check_empty = majority_label(self_check[:1])\n",
    "assert self_check_empty == None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_domains\"></a>\n",
    "## get_domains\n",
    "\n",
    "- Gets the domains for a given attribute\n",
    "- This is unique values that a column can take on\n",
    "\n",
    "* **data** List[List[]]: data used for classifying \n",
    "* **col_index** int: attribute index in data\n",
    "\n",
    "**returns** List[str]: returns list of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domains(data:List[List], col_index:int)->List[str]:\n",
    "    if len(data) <= 1:\n",
    "        return []\n",
    "    values = [row[col_index] for row in data[1:]]\n",
    "    unique_vals = set(values)\n",
    "    return unique_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = get_domains(self_check, 0 )\n",
    "\n",
    "#verify a list of unique values is returned. No dups\n",
    "assert len(shapes) == 2\n",
    "\n",
    "#verify the correct values are present\n",
    "for s in shapes:\n",
    "    assert s in [\"round\",\"square\"]\n",
    "\n",
    "# verify empty list is returned if data contains no data\n",
    "empty_test = get_domains([],4)\n",
    "assert empty_test == []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id3\"></a>\n",
    "## id3\n",
    "\n",
    "- Produced a decision tree using the id3 algorithm\n",
    "- This is a recursive way to create a decision tree\n",
    "\n",
    "* **data** List[List[]]: data used for classifying \n",
    "* **attributes** List[str]: attribute names\n",
    "* **current_depth** int: current depth of the tree\n",
    "* **depth_limit** int: number of levels that the tree can take on\n",
    "* **default** str: default value that the tree will have when making a leaf\n",
    "\n",
    "**returns** dict: nested dictionary of a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3(data:List[List], attributes:List[str], current_depth:int, depth_limit:int, default:str)->dict:\n",
    "    if len(data) == 1:\n",
    "        return default\n",
    "    if homogen(data) != None:\n",
    "        return homogen(data)\n",
    "    if len(attributes) == 0 or current_depth > depth_limit:\n",
    "        return majority_label(data)\n",
    "    node_name, node_index = best_attribute(data, attributes)\n",
    "    node = {}\n",
    "    children = {}\n",
    "    default = majority_label(data)\n",
    "    for value in get_domains(data, node_index): # todo make a unique domain\n",
    "        subset = [ row for row in data if row[node_index] == value or row[node_index] == node_name] \n",
    "        remaining_att = deepcopy(attributes) \n",
    "        remaining_att.remove(node_name)\n",
    "        child = id3(subset,remaining_att, current_depth +1, depth_limit, default)\n",
    "        children[value] = child\n",
    "    children[\"?\"] = id3(data,remaining_att, current_depth +1, depth_limit, default)\n",
    "    node[node_name] = children\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_check_cols = [\"Size\",\"Shape\",\"Color\"]\n",
    "self_check_tree = id3(self_check, self_check_cols, 1, 3, \"no\")\n",
    "\n",
    "# verify a list is returned\n",
    "assert isinstance(self_check_tree, dict)\n",
    "\n",
    "#verify a nest dict is returned. Root should make the dict len of one\n",
    "assert len(self_check_tree) == 1 \n",
    "\n",
    "# verify root is size\n",
    "assert list(self_check_tree.keys()) == [\"Size\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## train\n",
    "\n",
    "- Trains a dataset by creating a decision true\n",
    "\n",
    "* **training_data** List[List[]]: data used for classifying \n",
    "* **depth_limit** int: number of levels that the tree can take on\n",
    "\n",
    "\n",
    "**returns** dict: nested dictionary of a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data:List[List], depth_limit=None)->dict:\n",
    "   default = majority_label(training_data)\n",
    "   atttributes = training_data[0][:-1]\n",
    "\n",
    "   if depth_limit == None:\n",
    "      depth_limit = len(atttributes)\n",
    "   tree = id3(training_data, atttributes, 1,depth_limit, default)\n",
    "\n",
    "   return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_train = train(self_check)\n",
    "\n",
    "#verify a dictionary is returned\n",
    "assert isinstance(tree_train,dict)\n",
    "\n",
    "# verify tree match the test from id3\n",
    "assert tree_train == self_check_tree\n",
    "\n",
    "#verify depth limit works\n",
    "tree_train_dep1 = train(self_check,1)\n",
    "assert tree_train_dep1 == {'Size': {'small': 'no', 'large': 'yes', '?': 'no'}} # only one attribute in tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_label\"></a>\n",
    "## get_label\n",
    "\n",
    "- Classifying a single data point by using recursion to go through a decision tree\n",
    "\n",
    "* **data** List[]: data used for classifying. A single record\n",
    "* **cols** List[str]: attribute names\n",
    "* **tree_part** dict: part of a decision tree\n",
    "\n",
    "**returns** str: label to classify the data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(data:List, cols:List[str], tree_part:dict)->str:\n",
    "    for att_name,att_mapping in tree_part.items():\n",
    "        att_index = cols.index(att_name)\n",
    "        data_val = data[att_index]\n",
    "        if data_val in att_mapping: # is the test value in the att mapping\n",
    "            if isinstance(att_mapping[data_val], dict):\n",
    "                value = get_label(data, cols, att_mapping[data_val])\n",
    "            else:\n",
    "                value = att_mapping[data_val]\n",
    "        else:\n",
    "            # Missing data\n",
    "            if isinstance(att_mapping[\"?\"], dict):\n",
    "                value = get_label(data, cols, att_mapping[\"?\"])\n",
    "            else:\n",
    "                value = att_mapping[\"?\"]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test =  [ 'round', 'large', 'green', 'no']\n",
    "test_label = get_label(d_test, [ 'Shape', 'Size', 'Color', 'Safe?'], tree_train)\n",
    "\n",
    "# verify a string is returned\n",
    "assert isinstance(test_label, str)\n",
    "\n",
    "# verify a value from label field is returned\n",
    "assert test_label in [\"yes\",\"no\"]\n",
    "\n",
    "# verify can handle missing values \n",
    "d_test2=   [ 'round', 'med', 'green', 'no']\n",
    "test_label2 = get_label(d_test2, [ 'Shape', 'Size', 'Color', 'Safe?'], tree_train)\n",
    "assert isinstance(test_label2,str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"classify\"></a>\n",
    "## classify\n",
    "\n",
    "- Classifies each data point by calling a recursive function to read through a decision tree\n",
    "\n",
    "* **tree** dict: decision tree\n",
    "* **observations** List[List[]]: data used for classifying. Multiple records\n",
    "* **labeled** bool: indicator is data is labeled or not. Not sure why this was requested. Did not use\n",
    "\n",
    "**returns** List[str]: labels for each data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree:dict, observations:List[List], labeled=True)->List[str]:\n",
    "    if len(observations) == 0:\n",
    "        return []\n",
    "    cols = observations[0][:-1]\n",
    "    labels = []\n",
    "    for ob in observations[1:]:\n",
    "        label = get_label(ob, cols, tree)\n",
    "        labels.append(label)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_check_test_data =[[ 'Shape', 'Size', 'Color', 'Safe?'],\n",
    " ['round', 'small', 'blue', 'yes'],\n",
    " [ 'round', 'large', 'green', 'no']]\n",
    "\n",
    "l = classify(tree_train, self_check_test_data, labeled=True)\n",
    "\n",
    "#verify each observation is labeled\n",
    "assert len(l) == 2\n",
    "\n",
    "#verify labels are valid labels\n",
    "for lab in l:\n",
    "    assert lab in [\"yes\",\"no\"]\n",
    "\n",
    "#verify empty test data can be handled\n",
    "l2 = classify(tree_train, self_check_test_data[:][:1], labeled=True)\n",
    "assert l2 == []\n",
    "l3 = classify(tree_train, [], labeled=True)\n",
    "assert l3 == []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluate\"></a>\n",
    "## evaluate\n",
    "\n",
    "- returns the error rate of the predictions\n",
    "\n",
    "* **label** List[str]: list of actual labels for the data points\n",
    "* **prediction** List[str]: list of prediction labels from the model\n",
    "\n",
    "\n",
    "**returns** float: returns error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(label:list[str], prediction:list[str])->float:\n",
    "    n = len(label)\n",
    "    false_vals = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        if label[i] != prediction[i]:\n",
    "            false_vals += 1\n",
    "    return false_vals/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_test = [1,1,1,0]\n",
    "p_test = [0,0,0,0]\n",
    "\n",
    "# verify error rate is 0 if all match\n",
    "zero_rate = evaluate(l_test,l_test)\n",
    "assert zero_rate == 0\n",
    "\n",
    "#verify error rate doesnt match accuracy\n",
    "e_test = evaluate(l_test, p_test)\n",
    "assert e_test != 1/4\n",
    "\n",
    "# verify error rate is correct\n",
    "e_test = evaluate(l_test, p_test)\n",
    "assert e_test == 3/4\n",
    "\n",
    "# verify accuracy and error rate when half data is correct\n",
    "p_2 = [1,1,0,1]\n",
    "e_test2 = evaluate(l_test, p_2)\n",
    "accuracy = 2/4 # two correct predictions over 4 records\n",
    "assert e_test2 == accuracy "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cross_validate\"></a>\n",
    "## cross_validate\n",
    "\n",
    "- Performs cross validation to classify data using a decision tree\n",
    "- First splits the data into 10 folds\n",
    "- Then trains the model to build the decision tree\n",
    "- Then makes predictions on the test set\n",
    "- Then evaulate the model\n",
    "- Prints out the results for each fold\n",
    "\n",
    "* **data** List[List[]]: data used for classifying. Data is parsed\n",
    "* **col_names** List[str]: attribute names\n",
    "* **depth_limit** int: number of levels that the tree can take on\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(data:List[list],col_names:list[str], depth_limit:None):\n",
    "    folds = create_folds(data, 10)\n",
    "    for i in range(10):\n",
    "        train_data, test_data = create_train_test(folds, i,col_names)\n",
    "        tree = train(train_data, depth_limit)\n",
    "\n",
    "        pred_labels = classify(tree, test_data, labeled=True)\n",
    "        actual_labels = [row[-1] for row in test_data[1:]]\n",
    "\n",
    "        error_rate = evaluate(actual_labels, pred_labels)\n",
    "        error_rate = error_rate*100\n",
    "        print(\"Fold\", i, \"Error rate:\", error_rate, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Error rate: 0.0 %\n",
      "Fold 1 Error rate: 0.0 %\n",
      "Fold 2 Error rate: 0.0 %\n",
      "Fold 3 Error rate: 0.0 %\n",
      "Fold 4 Error rate: 0.0 %\n",
      "Fold 5 Error rate: 0.0 %\n",
      "Fold 6 Error rate: 0.0 %\n",
      "Fold 7 Error rate: 0.0 %\n",
      "Fold 8 Error rate: 0.0 %\n",
      "Fold 9 Error rate: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "cross_validate(data,mushroom_cols,4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pretty_print_tree\"></a>\n",
    "## pretty_print_tree\n",
    "\n",
    "- Prints the tree\n",
    "- Uses recursion to print the tree\n",
    "- Each level is indented\n",
    "\n",
    "* **tree** dict: decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_tree(tree:dict):\n",
    "    #print(json.dumps(self_check_tree, sort_keys=True, indent=2)) #still a dictionary format but spaced properly\n",
    "    def pretty_print_tree_recursive(tree, depth=0):\n",
    "        for k,v in tree.items():\n",
    "            if isinstance(v, dict):\n",
    "                print('\\t'*depth + k+\":\")\n",
    "                pretty_print_tree_recursive(v, depth+1)\n",
    "            else:\n",
    "                print('\\t'*depth + k + \":\" + v )\n",
    "    return pretty_print_tree_recursive(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:\n",
      "\tsmall:\n",
      "\t\tShape:\n",
      "\t\t\tround:\n",
      "\t\t\t\tColor:\n",
      "\t\t\t\t\tblue:no\n",
      "\t\t\t\t\tgreen:no\n",
      "\t\t\t\t\tred:yes\n",
      "\t\t\t\t\t?:no\n",
      "\t\t\tsquare:no\n",
      "\t\t\t?:\n",
      "\t\t\t\tColor:\n",
      "\t\t\t\t\tblue:no\n",
      "\t\t\t\t\tgreen:no\n",
      "\t\t\t\t\tred:no\n",
      "\t\t\t\t\t?:no\n",
      "\tlarge:\n",
      "\t\tColor:\n",
      "\t\t\tblue:no\n",
      "\t\t\tgreen:yes\n",
      "\t\t\tred:\n",
      "\t\t\t\tShape:\n",
      "\t\t\t\t\tround:yes\n",
      "\t\t\t\t\tsquare:no\n",
      "\t\t\t\t\t?:yes\n",
      "\t\t\t?:\n",
      "\t\t\t\tShape:\n",
      "\t\t\t\t\tround:yes\n",
      "\t\t\t\t\tsquare:yes\n",
      "\t\t\t\t\t?:yes\n",
      "\t?:\n",
      "\t\tColor:\n",
      "\t\t\tblue:no\n",
      "\t\t\tgreen:\n",
      "\t\t\t\tShape:\n",
      "\t\t\t\t\tround:no\n",
      "\t\t\t\t\tsquare:yes\n",
      "\t\t\t\t\t?:yes\n",
      "\t\t\tred:\n",
      "\t\t\t\tShape:\n",
      "\t\t\t\t\tround:yes\n",
      "\t\t\t\t\tsquare:no\n",
      "\t\t\t\t\t?:no\n",
      "\t\t\t?:\n",
      "\t\t\t\tShape:\n",
      "\t\t\t\t\tround:yes\n",
      "\t\t\t\t\tsquare:no\n",
      "\t\t\t\t\t?:no\n"
     ]
    }
   ],
   "source": [
    "# test out printing tree\n",
    "pretty_print_tree(self_check_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odor:\n",
      "\ts:p\n",
      "\tc:p\n",
      "\tf:p\n",
      "\tl:e\n",
      "\tm:p\n",
      "\tp:p\n",
      "\ta:e\n",
      "\ty:p\n",
      "\tn:\n",
      "\t\tspore-print-color:\n",
      "\t\t\tk:e\n",
      "\t\t\tb:e\n",
      "\t\t\to:e\n",
      "\t\t\tr:p\n",
      "\t\t\ty:e\n",
      "\t\t\tn:e\n",
      "\t\t\th:e\n",
      "\t\t\tw:\n",
      "\t\t\t\thabitat:\n",
      "\t\t\t\t\tg:e\n",
      "\t\t\t\t\tl:\n",
      "\t\t\t\t\t\tcap-color:\n",
      "\t\t\t\t\t\t\tn:e\n",
      "\t\t\t\t\t\t\tc:e\n",
      "\t\t\t\t\t\t\tw:p\n",
      "\t\t\t\t\t\t\ty:p\n",
      "\t\t\t\t\t\t\t?:e\n",
      "\t\t\t\t\tp:e\n",
      "\t\t\t\t\td:\n",
      "\t\t\t\t\t\tgill-size:\n",
      "\t\t\t\t\t\t\tb:e\n",
      "\t\t\t\t\t\t\tn:p\n",
      "\t\t\t\t\t\t\t?:p\n",
      "\t\t\t\t\tw:e\n",
      "\t\t\t\t\t?:\n",
      "\t\t\t\t\t\tgill-size:\n",
      "\t\t\t\t\t\t\tb:e\n",
      "\t\t\t\t\t\t\tn:p\n",
      "\t\t\t\t\t\t\t?:e\n",
      "\t\t\t?:\n",
      "\t\t\t\tcap-color:\n",
      "\t\t\t\t\tb:\n",
      "\t\t\t\t\t\tstalk-root:\n",
      "\t\t\t\t\t\t\tb:p\n",
      "\t\t\t\t\t\t\t?:e\n",
      "\t\t\t\t\tc:e\n",
      "\t\t\t\t\tg:e\n",
      "\t\t\t\t\tp:\n",
      "\t\t\t\t\t\thabitat:\n",
      "\t\t\t\t\t\t\tp:e\n",
      "\t\t\t\t\t\t\tg:p\n",
      "\t\t\t\t\t\t\tw:e\n",
      "\t\t\t\t\t\t\tm:p\n",
      "\t\t\t\t\t\t\t?:e\n",
      "\t\t\t\t\te:e\n",
      "\t\t\t\t\tr:e\n",
      "\t\t\t\t\ty:p\n",
      "\t\t\t\t\tn:\n",
      "\t\t\t\t\t\tstalk-surface-above-ring:\n",
      "\t\t\t\t\t\t\ts:e\n",
      "\t\t\t\t\t\t\tf:e\n",
      "\t\t\t\t\t\t\ty:e\n",
      "\t\t\t\t\t\t\tk:p\n",
      "\t\t\t\t\t\t\t?:e\n",
      "\t\t\t\t\tu:e\n",
      "\t\t\t\t\tw:\n",
      "\t\t\t\t\t\tbruises:\n",
      "\t\t\t\t\t\t\tf:e\n",
      "\t\t\t\t\t\t\tt:p\n",
      "\t\t\t\t\t\t\t?:e\n",
      "\t\t\t\t\t?:\n",
      "\t\t\t\t\t\tgill-color:\n",
      "\t\t\t\t\t\t\tk:e\n",
      "\t\t\t\t\t\t\th:e\n",
      "\t\t\t\t\t\t\tg:e\n",
      "\t\t\t\t\t\t\to:e\n",
      "\t\t\t\t\t\t\tp:e\n",
      "\t\t\t\t\t\t\te:e\n",
      "\t\t\t\t\t\t\tr:p\n",
      "\t\t\t\t\t\t\ty:e\n",
      "\t\t\t\t\t\t\tn:e\n",
      "\t\t\t\t\t\t\tu:e\n",
      "\t\t\t\t\t\t\tw:e\n",
      "\t\t\t\t\t\t\t?:e\n",
      "\t?:\n",
      "\t\tspore-print-color:\n",
      "\t\t\tk:\n",
      "\t\t\t\tgill-size:\n",
      "\t\t\t\t\tb:e\n",
      "\t\t\t\t\tn:\n",
      "\t\t\t\t\t\tpopulation:\n",
      "\t\t\t\t\t\t\ts:p\n",
      "\t\t\t\t\t\t\ty:e\n",
      "\t\t\t\t\t\t\tv:p\n",
      "\t\t\t\t\t\t\t?:p\n",
      "\t\t\t\t\t?:\n",
      "\t\t\t\t\t\tstalk-shape:\n",
      "\t\t\t\t\t\t\te:e\n",
      "\t\t\t\t\t\t\tt:e\n",
      "\t\t\t\t\t\t\t?:e\n",
      "\t\t\tu:e\n",
      "\t\t\tb:e\n",
      "\t\t\to:e\n",
      "\t\t\tr:p\n",
      "\t\t\ty:e\n",
      "\t\t\tn:\n",
      "\t\t\t\tgill-size:\n",
      "\t\t\t\t\tb:e\n",
      "\t\t\t\t\tn:\n",
      "\t\t\t\t\t\tpopulation:\n",
      "\t\t\t\t\t\t\ts:p\n",
      "\t\t\t\t\t\t\ty:e\n",
      "\t\t\t\t\t\t\tv:p\n",
      "\t\t\t\t\t\t\t?:p\n",
      "\t\t\t\t\t?:\n",
      "\t\t\t\t\t\tstalk-shape:\n",
      "\t\t\t\t\t\t\te:e\n",
      "\t\t\t\t\t\t\tt:e\n",
      "\t\t\t\t\t\t\t?:e\n",
      "\t\t\th:\n",
      "\t\t\t\tgill-size:\n",
      "\t\t\t\t\tb:p\n",
      "\t\t\t\t\tn:e\n",
      "\t\t\t\t\t?:\n",
      "\t\t\t\t\t\tstalk-root:\n",
      "\t\t\t\t\t\t\tb:p\n",
      "\t\t\t\t\t\t\t?:p\n",
      "\t\t\tw:\n",
      "\t\t\t\tgill-color:\n",
      "\t\t\t\t\tb:p\n",
      "\t\t\t\t\tg:e\n",
      "\t\t\t\t\tp:e\n",
      "\t\t\t\t\te:e\n",
      "\t\t\t\t\ty:p\n",
      "\t\t\t\t\tw:\n",
      "\t\t\t\t\t\thabitat:\n",
      "\t\t\t\t\t\t\tg:e\n",
      "\t\t\t\t\t\t\tl:e\n",
      "\t\t\t\t\t\t\tp:e\n",
      "\t\t\t\t\t\t\td:p\n",
      "\t\t\t\t\t\t\tw:e\n",
      "\t\t\t\t\t\t\t?:e\n",
      "\t\t\t\t\t?:\n",
      "\t\t\t\t\t\tring-number:\n",
      "\t\t\t\t\t\t\to:p\n",
      "\t\t\t\t\t\t\tn:p\n",
      "\t\t\t\t\t\t\tt:e\n",
      "\t\t\t\t\t\t\t?:p\n",
      "\t\t\t?:\n",
      "\t\t\t\tgill-color:\n",
      "\t\t\t\t\tu:\n",
      "\t\t\t\t\t\tbruises:\n",
      "\t\t\t\t\t\t\tf:p\n",
      "\t\t\t\t\t\t\tt:e\n",
      "\t\t\t\t\t\t\t?:e\n",
      "\t\t\t\t\tk:\n",
      "\t\t\t\t\t\tgill-size:\n",
      "\t\t\t\t\t\t\tb:e\n",
      "\t\t\t\t\t\t\tn:p\n",
      "\t\t\t\t\t\t\t?:e\n",
      "\t\t\t\t\tb:p\n",
      "\t\t\t\t\tg:\n",
      "\t\t\t\t\t\tstalk-root:\n",
      "\t\t\t\t\t\t\tb:p\n",
      "\t\t\t\t\t\t\tc:e\n",
      "\t\t\t\t\t\t\t?:p\n",
      "\t\t\t\t\t\t\te:e\n",
      "\t\t\t\t\to:e\n",
      "\t\t\t\t\tp:\n",
      "\t\t\t\t\t\tring-type:\n",
      "\t\t\t\t\t\t\tp:e\n",
      "\t\t\t\t\t\t\te:e\n",
      "\t\t\t\t\t\t\tf:e\n",
      "\t\t\t\t\t\t\tl:p\n",
      "\t\t\t\t\t\t\t?:e\n",
      "\t\t\t\t\te:e\n",
      "\t\t\t\t\tr:p\n",
      "\t\t\t\t\ty:\n",
      "\t\t\t\t\t\tcap-surface:\n",
      "\t\t\t\t\t\t\ts:e\n",
      "\t\t\t\t\t\t\ty:p\n",
      "\t\t\t\t\t\t\t?:e\n",
      "\t\t\t\t\tn:\n",
      "\t\t\t\t\t\tgill-size:\n",
      "\t\t\t\t\t\t\tb:e\n",
      "\t\t\t\t\t\t\tn:p\n",
      "\t\t\t\t\t\t\t?:e\n",
      "\t\t\t\t\th:\n",
      "\t\t\t\t\t\tstalk-root:\n",
      "\t\t\t\t\t\t\tb:p\n",
      "\t\t\t\t\t\t\te:e\n",
      "\t\t\t\t\t\t\t?:p\n",
      "\t\t\t\t\tw:\n",
      "\t\t\t\t\t\thabitat:\n",
      "\t\t\t\t\t\t\tg:e\n",
      "\t\t\t\t\t\t\tl:e\n",
      "\t\t\t\t\t\t\tm:e\n",
      "\t\t\t\t\t\t\tp:e\n",
      "\t\t\t\t\t\t\td:e\n",
      "\t\t\t\t\t\t\tu:p\n",
      "\t\t\t\t\t\t\tw:e\n",
      "\t\t\t\t\t\t\t?:e\n",
      "\t\t\t\t\t?:\n",
      "\t\t\t\t\t\tring-type:\n",
      "\t\t\t\t\t\t\tf:e\n",
      "\t\t\t\t\t\t\tl:p\n",
      "\t\t\t\t\t\t\tp:e\n",
      "\t\t\t\t\t\t\te:p\n",
      "\t\t\t\t\t\t\tn:p\n",
      "\t\t\t\t\t\t\t?:e\n"
     ]
    }
   ],
   "source": [
    "full_data_train = deepcopy(data) \n",
    "full_data_train.insert(0,mushroom_cols)\n",
    "full_tree = train(full_data_train, 4)\n",
    "pretty_print_tree(full_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "en605645",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
