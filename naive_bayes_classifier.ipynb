{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "1. Change the name of this file to be your JHED id as in `jsmith299.ipynb`. Because sure you use your JHED ID (it's made out of your name and not your student id which is just letters and numbers).\n",
    "2. Make sure the notebook you submit is cleanly and fully executed. I do not grade unexecuted notebooks.\n",
    "3. Submit your notebook back in Blackboard where you downloaded this file.\n",
    "\n",
    "*Provide the output **exactly** as requested*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "For this assignment you will be implementing and evaluating a Naive Bayes Classifier with the same data from last week:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "(You should have downloaded it).\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        No Pandas. The only acceptable libraries in this class are those contained in the `environment.yml`. No OOP, either. You can used Dicts, NamedTuples, etc. as your abstract data type (ADT) for the the tree and nodes.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "You'll first need to calculate all of the necessary probabilities using a `train` function. A flag will control whether or not you use \"+1 Smoothing\" or not. You'll then need to have a `classify` function that takes your probabilities, a List of instances (possibly a list of 1) and returns a List of Tuples. Each Tuple has the best class in the first position and a dict with a key for every possible class label and the associated *normalized* probability. For example, if we have given the `classify` function a list of 2 observations, we would get the following back:\n",
    "\n",
    "```\n",
    "[(\"e\", {\"e\": 0.98, \"p\": 0.02}), (\"p\", {\"e\": 0.34, \"p\": 0.66})]\n",
    "```\n",
    "\n",
    "when calculating the error rate of your classifier, you should pick the class label with the highest probability; you can write a simple function that takes the Dict and returns that class label.\n",
    "\n",
    "As a reminder, the Naive Bayes Classifier generates the *unnormalized* probabilities from the numerator of Bayes Rule:\n",
    "\n",
    "$$P(C|A) \\propto P(A|C)P(C)$$\n",
    "\n",
    "where C is the class and A are the attributes (data). Since the normalizer of Bayes Rule is the *sum* of all possible numerators and you have to calculate them all, the normalizer is just the sum of the probabilities.\n",
    "\n",
    "You will have the same basic functions as the last module's assignment and some of them can be reused or at least repurposed.\n",
    "\n",
    "`train` takes training_data and returns a Naive Bayes Classifier (NBC) as a data structure. There are many options including namedtuples and just plain old nested dictionaries. **No OOP**.\n",
    "\n",
    "```\n",
    "def train(training_data, smoothing=True):\n",
    "   # returns the Decision Tree.\n",
    "```\n",
    "\n",
    "The `smoothing` value defaults to True. You should handle both cases.\n",
    "\n",
    "`classify` takes a NBC produced from the function above and applies it to labeled data (like the test set) or unlabeled data (like some new data). (This is not the same `classify` as the pseudocode which classifies only one instance at a time; it can call it though).\n",
    "\n",
    "```\n",
    "def classify(nbc, observations, labeled=True):\n",
    "    # returns a list of tuples, the argmax and the raw data as per the pseudocode.\n",
    "```\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the classification result and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "Do not use anything else as evaluation metric or the submission will be deemed incomplete, ie, an \"F\". (Hint: accuracy rate is not the error rate!).\n",
    "\n",
    "`cross_validate` takes the data and uses 10 fold cross validation (from Module 3!) to `train`, `classify`, and `evaluate`. **Remember to shuffle your data before you create your folds**. I leave the exact signature of `cross_validate` to you but you should write it so that you can use it with *any* `classify` function of the same form (using higher order functions and partial application). If you did so last time, you can reuse it for this assignment.\n",
    "\n",
    "Following Module 3's discussion, `cross_validate` should print out the fold number and the evaluation metric (error rate) for each fold and then the average value (and the variance). What you are looking for here is a consistent evaluation metric cross the folds. You should print the error rates in terms of percents (ie, multiply the error rate by 100 and add \"%\" to the end).\n",
    "\n",
    "To summarize...\n",
    "\n",
    "Apply the Naive Bayes Classifier algorithm to the Mushroom data set using 10 fold cross validation and the error rate as the evaluation metric. You will do this *twice*. Once with smoothing=True and once with smoothing=False. You should follow up with a brief explanation for the similarities or differences in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import List, Dict, Tuple, Callable\n",
    "import random\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data ready\n",
    "mushroom_cols = [\n",
    "    \"cap-shape\"\n",
    "    ,\"cap-surface\"\n",
    "    ,\"cap-color\"\n",
    "    ,\"bruises\"\n",
    "    ,\"odor\"\n",
    "    ,\"gill-attachment\"\n",
    "    ,\"gill-spacing\"\n",
    "    ,\"gill-size\"\n",
    "    ,\"gill-color\"\n",
    "    ,\"stalk-shape\"\n",
    "    ,\"stalk-root\"\n",
    "    ,\"stalk-surface-above-ring\"\n",
    "    ,\"stalk-surface-below-ring\"\n",
    "    ,\"stalk-color-above-ring\"\n",
    "    ,\"stalk-color-below-ring\"\n",
    "    ,\"veil-type\"\n",
    "    ,\"veil-color\"\n",
    "    ,\"ring-number\"\n",
    "    ,\"ring-type\"\n",
    "    ,\"spore-print-color\"\n",
    "    ,\"population\"\n",
    "    ,\"habitat\"\n",
    "    ,\"edibility\"\n",
    "]\n",
    "\n",
    "self_check =[[ 'Shape', 'Size', 'Color', 'Safe?'],\n",
    " ['round', 'large', 'blue', 'no'],\n",
    " [ 'square', 'large', 'red', 'no'],\n",
    " ['round', 'large', 'green', 'yes'],\n",
    " ['square', 'large', 'green', 'yes'],\n",
    " [ 'square', 'large', 'green', 'yes'],\n",
    " [ 'square', 'large', 'green', 'yes'],\n",
    " ['round', 'large', 'red', 'yes'],\n",
    " [ 'round', 'large', 'red', 'yes'],\n",
    " [ 'round', 'small', 'blue', 'no'],\n",
    " ['square', 'small', 'blue', 'no'],\n",
    " ['round', 'small', 'green', 'no'],\n",
    " [ 'square', 'small', 'green', 'no'],\n",
    " ['square', 'small', 'red', 'no'],\n",
    " [ 'square', 'small', 'red', 'no'],\n",
    " ['round', 'small', 'red', 'yes']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"parse_data\"></a>\n",
    "## parse_data\n",
    "\n",
    "- Reads in a comma separated file into a nested list\n",
    "- Stores the label column as the very last column\n",
    "- Function mostly resued from mod 3\n",
    "\n",
    "* **file_name** str: path to where file is located\n",
    "* **class_index** int: index of where label field is in the file\n",
    "\n",
    "**returns** List[List[]]: data stored in a nest list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file_name: str, class_index:int) -> List[List]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = [value for value in line.rstrip().split(\",\")]\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    for row in data:\n",
    "        #swap\n",
    "        label = row[class_index]\n",
    "        row.pop(class_index)\n",
    "        row.append(label)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit tests\n",
    "data = parse_data(\"agaricus-lepiota.data\",0)\n",
    "\n",
    "# verify all observations are present\n",
    "assert len(data) ==8124 \n",
    "\n",
    "#verify all attributes and class cols are present\n",
    "assert len(data[0]) == 23\n",
    "\n",
    "# verify moved class/label col is last column\n",
    "for row in data[1:]:\n",
    "    assert row[0] not in ['p','e'] # first col is cap-shape, doesnt have values e or p\n",
    "    assert row[-1] in ['p','e'] # label/class only takes e or p value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create_folds\"></a>\n",
    "## create_folds\n",
    "\n",
    "- Resued from mod 3\n",
    "- Creates folds from the data. Fold number based on parameter\n",
    "\n",
    "* **xs** List[List[]]: list of to perform cross validation on\n",
    "* **n** int: number of folds\n",
    "\n",
    "**returns** List[List[float]]: normalized data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(xs: List[List], n: int) -> List[List[List]]:\n",
    "    k, m = divmod(len(xs), n)\n",
    "    # be careful of generators...\n",
    "    return list(xs[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make unit tests\n",
    "folds = create_folds(data, 10)\n",
    "\n",
    "#verify a list is returned\n",
    "assert type(folds) == list\n",
    "\n",
    "# no other unit tests since this was used in mod 3. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create_train_test\"></a>\n",
    "## create_train_test\n",
    "\n",
    "- Mostly resused function from Mod 3\n",
    "- Creates training and test data based on folds\n",
    "- also for both training and test set, the column names are added to the first(index 0) row of the list\n",
    "\n",
    "* **folds**: List[List[List]]: data to split\n",
    "* **index** : index of fold for splitting\n",
    "* **cols_names**: list of column names\n",
    "\n",
    "**returns** Tuple[List[List], List[List]]: returns training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(folds: List[List[List]], index: int, cols_names:List[str]) -> Tuple[List[List], List[List]]:\n",
    "    training = []\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = training + fold\n",
    "    # add column names\n",
    "    training.insert(0,cols_names)\n",
    "    test_copy = deepcopy(test)\n",
    "    test_copy.insert(0,cols_names)\n",
    "    return training, test_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mush, test_mush = create_train_test(folds, 0,mushroom_cols)\n",
    "\n",
    "# verify first row is the column names\n",
    "assert train_mush[0] == test_mush[0] == mushroom_cols\n",
    "\n",
    "# verify train is 9/10 of data\n",
    "assert len(train_mush) == math.floor(len(data) * 9/10) +1 # round for the col name\n",
    "\n",
    "\n",
    "# train and test should be the same size as data\n",
    "assert len(test_mush) + len(train_mush) == len(data) +2 # 2 col rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluate\"></a>\n",
    "## evaluate\n",
    "\n",
    "- returns the error rate of the predictions\n",
    "\n",
    "* **label** List[str]: list of actual labels for the data points\n",
    "* **prediction** List[str]: list of prediction labels from the model\n",
    "\n",
    "\n",
    "**returns** float: returns error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(label:list[str], prediction:list[str])->float:\n",
    "    n = len(label)\n",
    "    false_vals = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        if label[i] != prediction[i]:\n",
    "            false_vals += 1\n",
    "    return false_vals/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_test = [1,1,1,0]\n",
    "p_test = [0,0,0,0]\n",
    "\n",
    "# verify error rate is 0 if all match\n",
    "zero_rate = evaluate(l_test,l_test)\n",
    "assert zero_rate == 0\n",
    "\n",
    "#verify error rate doesnt match accuracy\n",
    "e_test = evaluate(l_test, p_test)\n",
    "assert e_test != 1/4\n",
    "\n",
    "# verify error rate is correct\n",
    "e_test = evaluate(l_test, p_test)\n",
    "assert e_test == 3/4\n",
    "\n",
    "# verify accuracy and error rate when half data is correct\n",
    "p_2 = [1,1,0,1]\n",
    "e_test2 = evaluate(l_test, p_2)\n",
    "accuracy = 2/4 # two correct predictions over 4 records\n",
    "assert e_test2 == accuracy "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_label_count\"></a>\n",
    "## get_label_count\n",
    "\n",
    "- Gets a count of each label in a data set\n",
    "\n",
    "* **data** List[List[]]: data used for classifying. Data is parsed\n",
    "* **index** int: position of label column in data\n",
    "\n",
    "\n",
    "**returns** dict: nested dictionary count per label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_count(data:List[List], index:-1)->dict:\n",
    "    label_cnt = {}\n",
    "    labels = [ rw [index] for rw in data]\n",
    "    unique_labels = set(labels)\n",
    "\n",
    "    for label in unique_labels:\n",
    "        class_list = [ c for c in labels if c == label]\n",
    "        label_cnt[label] = len(class_list)\n",
    "    \n",
    "    return label_cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cnt_test = get_label_count(self_check[1:],-1)\n",
    "\n",
    "# verify get label count returns a dictionary of 2 class\n",
    "assert len(label_cnt_test) == 2\n",
    "\n",
    "# verify the value is a integer, the count\n",
    "for k,v in label_cnt_test.items():\n",
    "    assert isinstance(v, int)\n",
    "\n",
    "# verify the counts are correct\n",
    "assert label_cnt_test == {'yes': 7, 'no': 8}\n",
    "\n",
    "#verify empty data doesnt cause the function to break. Emtpy dictionary should be returned\n",
    "cnt_test2 = get_label_count([],1)\n",
    "assert cnt_test2 == {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"get_condition_prob\"></a>\n",
    "## get_condition_prob\n",
    "\n",
    "- Gets the conditional probablility of each value for a given feature\n",
    "\n",
    "* **data** List[List[]]: data used for classifying. Data is parsed\n",
    "* **col_index** int: position of feature in data\n",
    "* **label_cnt** dict: count of each label value in all of data\n",
    "* **smoothing** bool: indicating to use smoothing (+1) logic or not\n",
    "\n",
    "**returns** dict: conditional probabilities for each value for a given feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_condition_prob(data:List[List], col_index:int,label_cnt:dict,smoothing:bool)->dict:\n",
    "    val_prob = {}\n",
    "    col_values = [rw[col_index] for rw in data] \n",
    "    unique_cols = set(col_values)\n",
    "    for col_val in unique_cols:\n",
    "        subset = [rw for rw in data if rw[col_index] == col_val]\n",
    "        subset_cnt = get_label_count(subset, -1)\n",
    "        label_prob = {}\n",
    "        for label in label_cnt:\n",
    "            if label not in subset_cnt:\n",
    "                s_cnt = 0\n",
    "            else:\n",
    "                s_cnt = subset_cnt[label]\n",
    "            if smoothing:\n",
    "                prob = (s_cnt + 1) / (label_cnt[label] + 1)\n",
    "            else:\n",
    "                prob = s_cnt /label_cnt[label]\n",
    "            label_prob[label] = round(prob,3)\n",
    "        val_prob[col_val] = label_prob\n",
    "    return val_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_check_shape = get_condition_prob(self_check[1:], 0,label_cnt_test,True)\n",
    "self_check_shape\n",
    "\n",
    "# verify result is a dictionary with unique values in column\n",
    "assert len(self_check_shape) == 2\n",
    "\n",
    "# verify the each col value has 2 values (the number of labels in data set)\n",
    "for col in self_check_shape:\n",
    "    assert len(self_check_shape[col]) == 2\n",
    "\n",
    "# verify the values are as expected\n",
    "assert self_check_shape == {'square': {'yes': 0.5, 'no': 0.667}, 'round': {'yes': 0.625, 'no': 0.444}}\n",
    "\n",
    "#verify function can  handle values that dont have both labels (color=blue)\n",
    "color = get_condition_prob(self_check[1:], 2,label_cnt_test,True)\n",
    "assert color[\"blue\"] == {'yes': 0.125, 'no': 0.444}\n",
    "color2 = get_condition_prob(self_check[1:], 2,label_cnt_test,False)\n",
    "assert color2[\"blue\"] == {'no': 0.375, 'yes': 0}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## train\n",
    "\n",
    "- Creates a Naive Bayes Classifier model\n",
    "- What is returned is a dictionary probabilities used to classify a test point\n",
    "\n",
    "* **training_data** List[List[]]: data used for classifying. Data is parsed\n",
    "* **smoothing** bool: indicating to use smoothing (+1) logic or not\n",
    "\n",
    "\n",
    "**returns** dict: nested dictionary probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data:List[List], smoothing=True)->dict:\n",
    "    probs = {}\n",
    "    p_c ={}\n",
    "    if len(training_data) <= 1:\n",
    "        return {}\n",
    "    label_cnt = get_label_count(training_data[1:],-1)\n",
    "    \n",
    "    # get p(c)\n",
    "    for label in label_cnt:\n",
    "        p_c[label] = round(label_cnt[label] / (len(training_data) - 1),3) # exclude the column names\n",
    "    probs[training_data[0][-1]] = p_c\n",
    "\n",
    "    for i in range(len(training_data[0])-1):# loop through all features\n",
    "        probs[training_data[0][i]] = get_condition_prob(training_data[1:], i,label_cnt,smoothing)\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_check_probs = train(self_check, smoothing=True)\n",
    "self_check_probs2 = train(self_check, False)\n",
    "\n",
    "# verify the number of keys is number of columns\n",
    "assert len(self_check_probs) == len(self_check[0])\n",
    "assert len(self_check_probs2) == len(self_check[0])\n",
    "\n",
    "#verify label key is not a nested dict, but other cols are nested\n",
    "for k in self_check_probs:\n",
    "    for k2 in self_check_probs[k]:\n",
    "        if k == \"Safe?\":\n",
    "            assert isinstance(self_check_probs[k][k2], float)\n",
    "        else:\n",
    "            isinstance(self_check_probs[k][k2], dict)\n",
    "\n",
    "#verify function returns empty dictionary if data is empty\n",
    "empty_check = train([], smoothing=True)\n",
    "assert empty_check == {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_prob_dist\"></a>\n",
    "## get_prob_dist\n",
    "\n",
    "- Classifies a single observation using naive bayes classifier\n",
    "- What is returned is a the label with the probability distribution\n",
    "\n",
    "* **probs** dict: probabilities relating to the test point\n",
    "* **label** str: name of the label field\n",
    "\n",
    "\n",
    "**returns** Tuple[str, dict]: predicted label, probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_dist(probs:dict, label:str)->Tuple[str, dict]:\n",
    "    prob_dist = {}\n",
    "    label_vals = set(probs[label].keys())\n",
    "    col_names = set(probs.keys()) - {label}\n",
    "\n",
    "    for l_val in label_vals:\n",
    "        prod = probs[label][l_val]\n",
    "        for col in col_names:\n",
    "            prod *= probs[col][l_val]\n",
    "        prob_dist[l_val] = prod\n",
    "    \n",
    "    norm = sum(prob_dist.values()) # normalize\n",
    "    for k in prob_dist:\n",
    "        prob_dist[k] = round((prob_dist[k] / norm), 3)\n",
    "    # get arg max\n",
    "    pred_label = max(prob_dist, key =prob_dist.get)\n",
    "    \n",
    "    return (pred_label, prob_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = {'Safe?': {'yes': 0.467, 'no': 0.533},\n",
    " 'Shape': {'yes': 0.5, 'no': 0.667},\n",
    " 'Size':  {'yes': 0.875, 'no': 0.333},\n",
    " 'Color': {'yes': 0.5, 'no': 0.444}}\n",
    "test_prob_result = get_prob_dist(test_probs, \"Safe?\")\n",
    "\n",
    "#verify tuple is returned\n",
    "assert isinstance(test_prob_result,tuple)\n",
    "\n",
    "#verify first val is yes, the predicted label\n",
    "assert isinstance(test_prob_result[0], str)\n",
    "assert test_prob_result[0] == \"yes\"\n",
    "\n",
    "#verify second item is probability distribution\n",
    "assert isinstance(test_prob_result[1], dict)\n",
    "\n",
    "#verify probability distribution is the right size\n",
    "assert len(test_prob_result[1]) == 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"classify\"></a>\n",
    "## classify\n",
    "\n",
    "- Classifies each observation using the naive bayes classifier\n",
    "\n",
    "* **nbc** dict: probabilities of the naive bayes classifier\n",
    "* **observations** list[list]: data to make predictions on. Data is parsed\n",
    "* **labeled** bool: indicator if observations have a label field present or not\n",
    "\n",
    "\n",
    "**returns** List[Tuple[str, dict]]: list of predicted labels, probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(nbc:dict, observations:list[list], labeled=True)->list:\n",
    "    results = []\n",
    "    if labeled:\n",
    "        label = observations[0][-1]\n",
    "        observations = [rw[:-1] for rw in observations]\n",
    "        col_labels = observations[0]\n",
    "    else:\n",
    "        col_labels = observations[0]\n",
    "        label = [k for k in nbc if k not in col_labels]\n",
    "        label = label[0]\n",
    "    for ob in observations[1:]:\n",
    "        ob_nbc = {}\n",
    "        ob_nbc[label] = nbc[label] # probs for the observation\n",
    "        for col_index in range(len(ob)):\n",
    "            col_key = col_labels[col_index]\n",
    "            feature_prob = nbc[col_key][ob[col_index]]\n",
    "            ob_nbc[col_key] = feature_prob\n",
    "        prob_dist = get_prob_dist(ob_nbc, label)\n",
    "        results.append(prob_dist) \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_check_test_example = [self_check[0]]+ [[\"square\",\"large\",\"red\", \"yes\"]]\n",
    "test_classify = classify(self_check_probs, self_check_test_example, True)\n",
    "\n",
    "#verify a list of tuples is returned\n",
    "assert isinstance(test_classify, list)\n",
    "assert isinstance(test_classify[0], tuple)\n",
    "\n",
    "#verify the result is as expected. Matches prob returned from get_prob_dist\n",
    "assert test_classify[0] == test_prob_result\n",
    "\n",
    "# verify the same result returns if label is false\n",
    "self_check_test_example2 = [['Shape', 'Size', 'Color'], ['square', 'large', 'red']]\n",
    "test_classify2 = classify(self_check_probs, self_check_test_example2, False)\n",
    "assert test_classify2[0] == test_prob_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cross_validate\"></a>\n",
    "## cross_validate\n",
    "\n",
    "- Performs cross validation to classify data using a naive bayes classifier\n",
    "- First splits the data into 10 folds\n",
    "- Then trains the model to build naives bayes classifier (probabilities)\n",
    "- Then makes predictions on the test set\n",
    "- Then evaulate the model\n",
    "- Prints out the results for each fold\n",
    "\n",
    "* **data** List[List[]]: data used for classifying. Data is parsed\n",
    "* **col_names** List[str]: attribute names\n",
    "* **smoothing** bool: indicating to use smoothing (+1) logic or not\n",
    "* **labeled** bool: indicator if observations have a label field present or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(data:List[list],col_names:list[str], smoothing=True, labeled=True):\n",
    "    folds = create_folds(data, 10)\n",
    "    for i in range(10):\n",
    "        train_data, test_data = create_train_test(folds, i,col_names)\n",
    "        nbc = train(train_data, smoothing)\n",
    "\n",
    "        preds = classify(nbc, test_data, labeled)\n",
    "        actual_labels = [row[-1] for row in test_data[1:]]\n",
    "        pred_labels = [ pred[0] for pred in preds]\n",
    "\n",
    "        error_rate = evaluate(actual_labels, pred_labels)\n",
    "        error_rate = error_rate * 100\n",
    "        print(\"Fold\", i+1, \"Error rate:\", round(error_rate,3), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mushroom 10 Fold Cross Validation with Smoothing\n",
      "Fold 1 Error rate: 0.369 %\n",
      "Fold 2 Error rate: 0.123 %\n",
      "Fold 3 Error rate: 0.246 %\n",
      "Fold 4 Error rate: 0.246 %\n",
      "Fold 5 Error rate: 0.0 %\n",
      "Fold 6 Error rate: 0.616 %\n",
      "Fold 7 Error rate: 0.123 %\n",
      "Fold 8 Error rate: 0.369 %\n",
      "Fold 9 Error rate: 0.493 %\n",
      "Fold 10 Error rate: 0.862 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Mushroom 10 Fold Cross Validation with Smoothing\")\n",
    "cross_validate(data,mushroom_cols, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mushroom 10 Fold Cross Validation without Smoothing\n",
      "Fold 1 Error rate: 0.369 %\n",
      "Fold 2 Error rate: 0.123 %\n",
      "Fold 3 Error rate: 0.246 %\n",
      "Fold 4 Error rate: 0.123 %\n",
      "Fold 5 Error rate: 0.0 %\n",
      "Fold 6 Error rate: 0.493 %\n",
      "Fold 7 Error rate: 0.123 %\n",
      "Fold 8 Error rate: 0.369 %\n",
      "Fold 9 Error rate: 0.493 %\n",
      "Fold 10 Error rate: 0.985 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Mushroom 10 Fold Cross Validation without Smoothing\")\n",
    "cross_validate(data,mushroom_cols, False, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Results\n",
    "\n",
    "- Both models the smoothing vs the non smoothing had nearly the same results. \n",
    "- The error rate is very low, but these models did not perform as well as the decision tree models (mod 8)\n",
    "- The non smoothing model performed better (lower error rate) on two folds. \n",
    "- The smoothing model performed better on one fold. Otherwise, the error rate was the same between the two models. \n",
    "- The reason the non smoothing model might have performed better might be because there was no missing values in the data\n",
    "    - Missing values as in the test data had values not found in the training data\n",
    "    - Like test had ring_number = 2, but training only had ring_number = none and 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "en605645",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
